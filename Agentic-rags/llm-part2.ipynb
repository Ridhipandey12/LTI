{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3b4926f2-defa-451e-8fd6-a3fc827901c8",
   "metadata": {},
   "source": [
    "Zero-shot: Direct task (e.g., “What is KYC?”).\n",
    "\n",
    "Few-shot: Provide examples (e.g., “Q: What is a loan? A: … Q: What is KYC? A:”).\n",
    "\n",
    "Chain-of-Thought (CoT): Encourage step-by-step reasoning (e.g., “Let’s think through KYC verification…”).\n",
    "\n",
    "Best Practices:\n",
    "Clear instructions (e.g., “Answer in one sentence”).\n",
    "Contextual examples for few-shot.\n",
    "CoT for complex tasks (e.g., fraud detection).\n",
    "Corporate Relevance: Reduces training costs, enables quick deployment.\n",
    "Use Case: BFSI chatbot answering “What is KYC?” with few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fa5e5-9a03-4de0-9598-5e36125a36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "prompt = \"Q: What is a loan? A: A loan is borrowed money repaid with interest. Q: What is KYC? A:\"\n",
    "response = llm(prompt, max_length=100)[0][\"generated_text\"]\n",
    "print(\"Response:\", response)  # Know Your Customer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b8d56-84fe-4eeb-a363-dc887fbb0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires OpenAI API key\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key\"\n",
    "prompt = \"Q: What is a loan? A: A loan is borrowed money repaid with interest. Q: What is KYC? A:\"\n",
    "response = openai.Completion.create(model=\"gpt-3.5-turbo\", prompt=prompt, max_tokens=50)\n",
    "print(\"Response:\", response.choices[0].text)  # Know Your Customer..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4de15f01-fb61-49c6-a329-2eb7ce746a81",
   "metadata": {},
   "source": [
    "Tokens:\n",
    "Definition: Units of text (words, subwords, punctuation) that LLMs process.\n",
    "Example: “Bank offers loan” → Tokens: [“Bank”, “offers”, “loan”] (word-based) or [“Ban”, “##k”, “offer”, “##s”, “loan”] (subword).\n",
    "Why It Matters: LLMs convert text to token IDs (e.g., “Bank” → 1234) for numerical processing."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8981c7c-4ab1-48d5-9283-fe9f39e6b878",
   "metadata": {},
   "source": [
    "Tokenizers:\n",
    "Definition: Algorithms that split text into tokens and map them to IDs.\n",
    "\n",
    "    Types:\n",
    "Word-Based: Splits on spaces (e.g., NLTK, spaCy). Limited by vocabulary size.\n",
    "\n",
    "Byte-Pair Encoding (BPE): Merges frequent character pairs (used in GPT). Handles rare words (e.g., “antibiotic” → “anti” + “biotic”).\n",
    "\n",
    "WordPiece: Similar to BPE, used in BERT (e.g., “playing” → “play” + “##ing”).\n",
    "\n",
    "SentencePiece: Language-agnostic, used in T5, Mistral.\n",
    "\n",
    "Example: BPE on “Bank offers loan” → [“Bank”, “offer”, “##s”, “loan”], where “##s” is a subword.\n",
    "\n",
    "Context Window:\n",
    "\n",
    "Definition: Maximum number of tokens an LLM can process at once (e.g., 512 for BERT, 4096 for GPT-3).\n",
    "\n",
    "Example: “Bank offers loan…” with 10 tokens fits in a 512-token window, but a 5000-token report needs truncation or chunking.\n",
    "\n",
    "Why It Matters: Limits input/output length in BFSI contract analysis or IT log processing.\n",
    "\n",
    "Use Case:\n",
    "BFSI: Tokenizing “HDFC Bank offers low-interest loan” for sentiment analysis.\n",
    "Healthcare: Tokenizing “Diabetes mellitus treatment” for Q&A.\n",
    "IT: Processing logs within a context window for error detection.\n",
    "\n",
    "Code Demo (Hugging Face GPT-2 Tokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8737b-6fa3-47bf-acbf-ab00e03839d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "text = \"HDFC Bank offers low-interest loan\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer(text)['input_ids']\n",
    "print(\"Tokens:\", tokens)  # ['HD', 'FC', 'Bank', 'offers', 'low', '-', 'interest', 'loan']\n",
    "print(\"Token IDs:\", token_ids)  # [e.g., 1234, 5678, ...]\n",
    "print(\"Token Count:\", len(tokens))  # 8 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf728185-76bc-45e0-bd10-89829665ed63",
   "metadata": {},
   "source": [
    "#### Embeddings and Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4345986-a4b0-4e39-80ea-c4c450cf4a49",
   "metadata": {},
   "source": [
    "Embeddings: Definition: Dense vectors (e.g., 512D) representing tokens’ meanings.\n",
    "\n",
    "    Types:\n",
    "\n",
    "Word2Vec/FastText: Static embeddings (Module 2), non-contextual.\n",
    "\n",
    "BERT Embeddings: Contextual, token’s vector depends on sentence (e.g., “bank” in “Bank offers loan” vs. “River bank”).\n",
    "\n",
    "OpenAI Embeddings: High-dimensional (e.g., 1536D for text-embedding-ada-002), used for similarity, search.\n",
    "\n",
    "Sentence-BERT: Sentence-level embeddings for tasks like document similarity.\n",
    "\n",
    "Example: “Bank” → [0.1, -0.2, …] in BERT, adjusted by context.\n",
    "\n",
    "Why It Matters: Enables semantic understanding (e.g., “loan” ≈ “credit”).\n",
    "\n",
    "Parameters:\n",
    "\n",
    "Definition: Learnable weights in LLMs (e.g., attention matrices, FFN weights, embeddings).\n",
    "\n",
    "Scale: 110M (BERT-base), 124M (GPT-2), 175B (GPT-3).\n",
    "\n",
    "Example: BERT’s 110M parameters include token embeddings (30k vocab × 768D) and transformer layers.\n",
    "\n",
    "Why It Matters: More parameters = more capacity, but higher compute cost.\n",
    "\n",
    "Use Case:\n",
    "BFSI: OpenAI embeddings for contract similarity (e.g., “loan agreement” vs. “mortgage”).\n",
    "Healthcare: Sentence-BERT for clustering medical reports.\n",
    "IT: BERT embeddings for log classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5751cc7-125c-4b94-b756-117023445ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "text = \"HDFC Bank offers loan\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.last_hidden_state  # [1, seq_len, 768]\n",
    "print(\"BERT Embeddings Shape:\", embeddings.shape)  # [1, 6, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385b399-ecda-47e0-b2cd-d08e457f2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires OpenAI API key\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key\"  # Replace with actual key\n",
    "text = \"HDFC Bank offers loan\"\n",
    "response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
    "embedding = response['data'][0]['embedding']\n",
    "print(\"OpenAI Embedding Length:\", len(embedding))  # 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05166a3b-ee6d-40f6-b217-cc7fc9b6eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text = \"HDFC Bank offers loan\"\n",
    "embedding = model.encode(text)\n",
    "print(\"Sentence-BERT Embedding Shape:\", embedding.shape)  # [384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364db3ff-ca56-46bd-a5eb-71035afdd5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "inputs = tokenizer(\"How to reset password?\", return_tensors=\"pt\")\n",
    "# Low temperature\n",
    "#outputs_low = model.generate(**inputs, max_length=50, temperature=0.2)\n",
    "#print(\"Low Temp:\", tokenizer.decode(outputs_low[0]))\n",
    "# High temperature\n",
    "outputs_high = model.generate(**inputs, max_length=50, temperature=1)\n",
    "print(\"High Temp:\", tokenizer.decode(outputs_high[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a594dda-6119-4550-90a3-efd488052555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "def generate_text(prompt, temperature):\n",
    "    # Load pre-trained GPT-2 model and tokenizer\n",
    "    model_name = 'gpt2'\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and return\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Once upon a time in a distant galaxy,\"\n",
    "\n",
    "# Low temperature (more deterministic)\n",
    "low_temp_output = generate_text(prompt, temperature=0.3)\n",
    "\n",
    "# High temperature (more creative/random)\n",
    "high_temp_output = generate_text(prompt, temperature=1.0)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n--- Low Temperature Output (0.3) ---\\n\")\n",
    "print(low_temp_output)\n",
    "\n",
    "print(\"\\n--- High Temperature Output (1.0) ---\\n\")\n",
    "print(high_temp_output)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9247f766-a5cb-483e-8cb7-1886d692fd55",
   "metadata": {},
   "source": [
    "LLM Architecture:\n",
    "\n",
    "Base: Transformers (attention, FFNs, positional encoding, normalization).\n",
    "Scaling: 100M–175B parameters, 12–96 layers, 12–128 attention heads.\n",
    "Example: GPT-2 (124M parameters) generates text autoregressively.\n",
    "\n",
    "Variants:\n",
    "\n",
    "BERT: Encoder-only, bidirectional, for classification/NER.\n",
    "DistilBERT: Lightweight BERT (66M parameters, 40% smaller).\n",
    "GPT: Decoder-only, autoregressive, for generation.\n",
    "Mistral: Efficient decoder-only (e.g., Mixtral 8x7B, mixture-of-experts).\n",
    "T5: Encoder-decoder, text-to-text framework.\n",
    "BART: Encoder-decoder, denoising for summarization.\n",
    "\n",
    "Use Case:\n",
    "BFSI: BERT for sentiment, T5 for contract translation.\n",
    "Healthcare: DistilBERT for Q&A, BART for report summarization.\n",
    "IT: GPT for support, Mistral for efficient deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69208833-329d-48ee-bb48-261239216b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(\"HDFC Bank offers loan\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(\"BERT Sentiment:\", torch.softmax(outputs.logits, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde6699-3f74-4b4f-8088-99b33b6befdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires OpenAI API key\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key\"\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    prompt=\"Generate a response for: HDFC Bank offers loan\",\n",
    "    max_tokens=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"GPT-3.5 Response:\", response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8706489-9e1b-4e71-9ac8-b296fca41582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires OpenAI API key\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=\"your-api-key\")\n",
    "vector_store = FAISS.from_texts([\"Diabetes: manage with insulin\"], OpenAIEmbeddings(api_key=\"your-api-key\"))\n",
    "retriever = vector_store.as_retriever()\n",
    "prompt = f\"Context: {{context}}\\nQuestion: {{question}}\"\n",
    "context = vector_store.similarity_search(\"diabetes\")[0].page_content\n",
    "response = llm.invoke(prompt.format(context=context, question=\"How to manage diabetes?\"))\n",
    "print(\"Response:\", response.content)  # Insulin..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "87662d14-b40b-4314-82da-e5d0e507eb16",
   "metadata": {},
   "source": [
    "What is RAG and Why Use It?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73d48aef-fbaf-43f3-9f89-4e7f17854ef7",
   "metadata": {},
   "source": [
    "RAG (Retrieval-Augmented Generation) is like having a smart librarian. It first searches (retrieves) relevant information from a document (like a PDF) and then generates an answer using an AI model (LLaMA).\n",
    "Retrieval: Finds the right info (e.g., loan rules from a banking PDF).\n",
    "Generation: Answers your question using that info (e.g., “What’s the loan interest rate?” → “5%”).\n",
    "This is great for BFSI (e.g., answering KYC questions), healthcare (e.g., patient record queries), or gaming (e.g., poker strategy).\n",
    "\n",
    "Why Use It?: It’s more accurate than just generating answers because it bases the answer on real data from your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf852bd9-9fd8-410f-9f3f-fdd981d9e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain (for RAG), PyPDF2 (to read PDFs), FAISS (to search text), and the updated langchain-ollama package\n",
    "#!conda install -c conda-forge langchain langchain-community pypdf2 faiss-cpu -y\n",
    "#!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73747e03-b5bd-478c-9cab-dc644bb1e724",
   "metadata": {},
   "source": [
    "!ollama pull mistral\n",
    "!ollama runb mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed1ba2-9603-4543-b981-3722c33d0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9439823-3b64-4f49-a97e-cc173ab51998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tools we’ll use\n",
    "import PyPDF2\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaEmbeddings  # Updated import to fix deprecation\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae9f69-87d1-44a6-a2e3-5e01b8d31805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for our PDF on E: drive\n",
    "pdf_path = \"E:\\\\l&w\\\\Policy.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef9018-884c-43dd-98e7-23770eba0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the PDF and extract its text\n",
    "reader = PyPDF2.PdfReader(pdf_path)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()\n",
    "\n",
    "# Show the extracted text\n",
    "print(\"Extracted Text:\", text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "340c925f-09c0-4af5-886d-b7269bed6c17",
   "metadata": {},
   "source": [
    "# Ensure Ollama is running (run this in a terminal first)\n",
    "# Command: ollama serve\n",
    "\n",
    "# Verify Mistral is available (you already pulled it, 4.1 GB)\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ed056-4170-477c-87e1-8900f31a9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07070ac-933f-4593-8991-1e501bfc8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce660cd-47d8-4e38-95a5-370ae5a06cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_text(text)\n",
    "\n",
    "# Turn the chunks into numbers (embeddings) using LLaMA\n",
    "#embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "\n",
    "# Store the chunks and their embeddings in FAISS for searching\n",
    "vector_store = FAISS.from_texts(documents, embeddings)\n",
    "\n",
    "# Show how many chunks we created\n",
    "print(\"Number of Chunks:\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6990d89-02f6-489a-9f0f-354667a1dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Mistral as our AI model\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Build the RAG system\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41c47a-42ec-4cec-9b53-0e46c077ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system with a question\n",
    "query = \"What is the interest rate for a loan?\"\n",
    "response = qa_chain.run(query)\n",
    "print(\"RAG Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5010b-fdee-466e-88a5-907654dca017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system with a question\n",
    "query = \"what is the criteria to take more than ten thousand in loan?\"\n",
    "response = qa_chain.run(query)\n",
    "print(\"RAG Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fd126-bb10-4b6a-ab63-d0e02477a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path1 = \"E:\\\\l&w\\\\Policy1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4941c1-a63e-4913-99df-1f757f4e6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the updated banking PDF\n",
    "reader = PyPDF2.PdfReader(pdf_path1)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()\n",
    "print(\"Updated Banking Text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60527d0a-d4e7-40c3-b09e-2c616443ac38",
   "metadata": {},
   "source": [
    "## Poker Assistant Using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6f952-80b4-4b95-bbd9-ba361d02c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PDF with poker rules\n",
    "poker_pdf_path = \"E:\\\\l&w\\\\poker_rules.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16657ae-c02c-4333-a72e-91220bb6a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the poker rules PDF\n",
    "poker_reader = PyPDF2.PdfReader(poker_pdf_path)\n",
    "poker_text = \"\"\n",
    "for page in poker_reader.pages:\n",
    "    poker_text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40583f-84f9-4f16-845b-2863acb51067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and embed the poker rules\n",
    "poker_documents = text_splitter.split_text(poker_text)\n",
    "poker_vector_store = FAISS.from_texts(poker_documents, embeddings)\n",
    "\n",
    "# Setup RAG for the poker assistant\n",
    "poker_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=poker_vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5aed9c-7953-4ce2-9b41-2e129dc969bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the poker assistant\n",
    "game_query = \"I have a Flush, should I bet or fold?\"\n",
    "poker_response = poker_qa_chain.run(game_query)\n",
    "print(\"Poker Assistant Response:\", poker_response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72668e09-cd30-46cc-8edd-983c02edc992",
   "metadata": {},
   "source": [
    "Configuring the retriever to fetch more chunks (top 4 instead of default 1-2) for better context.\n",
    "Adding a custom prompt to guide Mistral to give specific, meaningful answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421676c8-4861-4364-b84d-a6c5215ea8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Mistral as our AI model\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Create a custom prompt for better answers\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Use the following context to answer the question in a concise and specific way:\\nContext: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "# Build the RAG system for banking with improved retrieval\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 4}),  # Fetch top 4 chunks\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# Test with varied banking questions\n",
    "queries = [\n",
    "    \"What is the interest rate for a loan if my credit score is 800?\",\n",
    "    \"What do I need for a $15,000 loan?\",\n",
    "    \"What happens if I make a $60,000 transaction?\"\n",
    "]\n",
    "for query in queries:\n",
    "    response = qa_chain.run(query)\n",
    "    print(f\"Question: {query}\\nAnswer: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee2281-a97a-44e9-acd9-17a95631180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RAG system for poker\n",
    "poker_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=poker_vector_store.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# Test with varied poker questions\n",
    "poker_queries = [\n",
    "    \"I have a Flush, should I bet or fold?\",\n",
    "    \"Should I bluff with a High Card?\",\n",
    "    \"What should I do in a late position with One Pair?\"\n",
    "]\n",
    "for query in poker_queries:\n",
    "    poker_response = poker_qa_chain.run(query)\n",
    "    print(f\"Question: {query}\\nAnswer: {poker_response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606131dd-5837-4064-86f7-546e4ec5dea9",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2981c-d655-4daa-9439-99948d2f114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7e58f-da17-4a18-9d0a-be0c411e3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for LangGraph, CrewAI, and AutoGen\n",
    "#!pip install langgraph crewai autogen\n",
    "\n",
    "# Ensure Ollama is running (run in a terminal: ollama serve)\n",
    "# Pull LLaMA 3.1 (4.1 GB)\n",
    "# ollama pull llama3.1\n",
    "\n",
    "# Verify Mistral and LLaMA are available\n",
    "!ollama list\n",
    "\n",
    "# Import basic libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fac88-df1d-4a72-b24c-800bbde036b4",
   "metadata": {},
   "source": [
    "## LangGraph - Building Agent Workflows"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffe3b2a9-563c-4272-b23c-815ac4823fbc",
   "metadata": {},
   "source": [
    "LangGraph (part of LangChain) lets you create workflows where agents pass tasks to each other, like a flowchart. It’s great for structuring complex tasks.\n",
    "Example: In poker, one agent could process game logs, then pass the data to another agent to analyze trends.\n",
    "We’ll use Mistral to summarize poker game data, as it’s good at understanding structured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d9f37-167b-4b69-93c6-53e4b75343e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ffa3f-28d2-48a4-943f-9c42f85ba55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b53f52-37a6-4f50-ae62-b6ee12e3d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setx PATH \"%PATH%;C:\\Program Files\\Graphviz\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4028f-1369-499b-8658-de19c7836f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.llms import Ollama\n",
    "from typing import TypedDict, Annotated\n",
    "import pandas as pd\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "# Define the state (data that agents share)\n",
    "class GameState(TypedDict):\n",
    "    raw_data: str\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1501d58-f086-4d52-87e0-b02096feaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample poker game data\n",
    "data = pd.DataFrame({\n",
    "    \"player\": [\"P1\", \"P2\", \"P3\"],\n",
    "    \"action\": [\"bet 1000\", \"fold\", \"raise 5000\"],\n",
    "    \"hand\": [\"Flush\", \"Pair\", \"Straight\"]\n",
    "})\n",
    "data_path = \"poker_logs.csv\"\n",
    "data.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60471a5-7c47-48fb-88b4-aa91e1168758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1: Load and process data\n",
    "def process_data(state: GameState) -> GameState:\n",
    "    df = pd.read_csv(data_path)\n",
    "    state[\"raw_data\"] = df.to_string()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cad44-1713-4b0c-88e4-e6bec1ef2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 2: Summarize data using Mistral\n",
    "def summarize_data(state: GameState) -> GameState:\n",
    "    llm = Ollama(model=\"mistral\")\n",
    "    prompt = f\"Summarize this poker game data:\\n{state['raw_data']}\"\n",
    "    state[\"summary\"] = llm.invoke(prompt)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2e0d7-f586-4887-951b-1d174aa950a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workflow\n",
    "workflow = StateGraph(GameState)\n",
    "workflow.add_node(\"process_data\", process_data)\n",
    "workflow.add_node(\"summarize_data\", summarize_data)\n",
    "workflow.add_edge(\"process_data\", \"summarize_data\")\n",
    "workflow.add_edge(\"summarize_data\", END)\n",
    "workflow.set_entry_point(\"process_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c8149-a30a-499b-bc7c-d13f3d3b3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LangGraph workflow\n",
    "def visualize_workflow(workflow):\n",
    "    dot = Digraph(comment=\"LangGraph Workflow\")\n",
    "    dot.attr(rankdir=\"LR\")  # Left to right layout\n",
    "\n",
    "    # Add nodes\n",
    "    for node in workflow.nodes:\n",
    "        dot.node(node, node)\n",
    "\n",
    "    # Add entry point (safely access internal attribute)\n",
    "    dot.node(\"START\", \"START\", shape=\"circle\")\n",
    "\n",
    "    # Use protected member to get entry point (LangGraph stores it here)\n",
    "    entry_point = workflow._entry_point if hasattr(workflow, \"_entry_point\") else list(workflow.nodes)[0]\n",
    "    dot.edge(\"START\", entry_point)\n",
    "\n",
    "    # Add edges\n",
    "    for edge in workflow.edges:\n",
    "        dot.edge(edge[0], edge[1])\n",
    "\n",
    "    # Add END node\n",
    "    dot.node(\"END\", \"END\", shape=\"doublecircle\")\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987f7a3-d9b8-460f-9ea2-f9d6186b6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph\n",
    "graph = visualize_workflow(workflow)\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d1ac2-91b9-44a6-adac-355bd04b8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\"raw_data\": \"\", \"summary\": \"\"})\n",
    "print(\"LangGraph Result - Summary:\", result[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4501eee-a095-4624-b927-b02db9f0ebcf",
   "metadata": {},
   "source": [
    "## CrewAI - Multi-Agent Collaboration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5995dfe2-8261-456a-a94c-85dee7083bc2",
   "metadata": {},
   "source": [
    "CrewAI lets multiple agents work as a team, each with a specific role. Agents communicate and collaborate to complete tasks.\n",
    "Example: In poker, one agent could research hand rankings, while another suggests betting strategies."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f16b2a8-132b-47aa-a717-7f4387d8f0e2",
   "metadata": {},
   "source": [
    "#!pip uninstall crewai\n",
    "#!pip install crewai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777467f-14c9-494e-9ccd-7ff5e54dc3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859da0bb-a0f2-4ab5-9933-0a6f805bee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea10b44-95ba-4b06-9b96-4de45abf9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent\n",
    "from langchain_ollama import OllamaLLM  # ✅ Correct new import\n",
    "\n",
    "# Define agents\n",
    "researcher = Agent(\n",
    "    role=\"Poker Researcher\",\n",
    "    goal=\"Research poker hand rankings\",\n",
    "    backstory=\"Expert in poker rules\",\n",
    "    llm=OllamaLLM(model=\"mistral\")  # ✅ Matches your installed model\n",
    ")\n",
    "\n",
    "strategist = Agent(\n",
    "    role=\"Poker Strategist\",\n",
    "    goal=\"Suggest a betting strategy\",\n",
    "    backstory=\"Experienced poker player\",\n",
    "    llm=OllamaLLM(model=\"llama3.1\")  # ✅ Use the correct local model name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1cc1f-088b-45a3-901b-beecbc17c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-ollama --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540ffa6-294f-40b4-8d76-8664b6714024",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\")\n",
    "print(llm.invoke(\"What is the best starting poker hand?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f472ae-2f5f-48b0-b00d-738e39a44024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f77796-cb99-49e4-aa3d-dd6d60ab0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315df28-72df-4f1b-9f9f-52ae145ad460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tasks\n",
    "research_task = Task(\n",
    "    description=\"List poker hand rankings from highest to lowest\",\n",
    "    agent=researcher,\n",
    "    expected_output=\"A list of poker hand rankings\"\n",
    ")\n",
    "\n",
    "strategy_task = Task(\n",
    "    description=\"Suggest a betting strategy for a player with a Flush\",\n",
    "    agent=strategist,\n",
    "    expected_output=\"A betting strategy suggestion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c64d9-ac17-4dfe-85f8-9ab5daf737d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the crew\n",
    "crew = Crew(\n",
    "    agents=[researcher, strategist],\n",
    "    tasks=[research_task, strategy_task],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbc9f2-2f29-410a-a43a-a2760bf718b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the crew\n",
    "result = crew.kickoff()\n",
    "print(\"CrewAI Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733660c6-f49b-4436-9df5-33cb701257ca",
   "metadata": {},
   "source": [
    "## AutoGen - Agent Communication"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35f8be87-40af-40f4-b205-124c9e45f466",
   "metadata": {},
   "source": [
    "AutoGen enables agents to talk to each other and collaborate on tasks, like a group chat. It’s great for dynamic interactions.\n",
    "Example: In poker, agents could discuss a game scenario and agree on a strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb5337-0171-4cf8-8541-63f53c08ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e1bef-837a-481d-a1dc-28bff5f84aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from autogen import Assistant, UserProxyAgent, config_list_from_json\n",
    "\n",
    "# Configure LLaMA via Ollama\n",
    "config_list = {\"model\": \"llama3.1\",\"api_type\": \"ollama\",\"base_url\": \"http://localhost:11434\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0c257-2424-473d-8d12-a62d8ee76157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agents\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"Player\",\n",
    "    #human_in_the_loop=False,\n",
    "    llm_config=config_list\n",
    ")\n",
    "\n",
    "analyst = AssistantAgent(\n",
    "    name=\"Analyst\",\n",
    "    system_message=\"You are a poker analyst. Discuss game scenarios and suggest moves.\",\n",
    "    llm_config=config_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b1268-9e35-42bf-912e-e6b8700ce022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation\n",
    "user_proxy.initiate_chat(\n",
    "    analyst,\n",
    "    message=\"I have a Straight, and my opponent raised 5000. Should I call or fold?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e0aaa-1e32-4120-8ded-e59e3e7ab296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ag2[ollama]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65651c06-3e56-42dd-b8de-ebab896c4a2b",
   "metadata": {},
   "source": [
    "## Multi-Agent Project: Poker Game Analysis and Strategy System"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4f2c950-2bc8-4d6c-a11b-30d2a2738c5b",
   "metadata": {},
   "source": [
    "Analyze Data: Process poker game logs (data analytics).\n",
    "Predict Outcomes: Use ML to predict win probabilities.\n",
    "Generate Strategies: Use Mistral to suggest moves.\n",
    "Collaborate: Use LangGraph, CrewAI, and AutoGen to manage the workflow.\n",
    "This mirrors real-world gaming tasks: analyzing player behavior, predicting outcomes, and optimizing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bce195-1257-4a49-9fb4-585fb80f0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langgraph\n",
    "!pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de582a11-3bcf-4f78-a8c1-8fe8ef060459",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain\\chains\\__init__.py:93\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importer(name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain\\_api\\module_import.py:69\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImporting from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed top-level packages are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWED_TOP_LEVEL_PKGS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(new_module)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStore\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfigDict, Field, model_validator\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chain\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseCombineDocumentsChain\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstuff\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StuffDocumentsChain\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain\\chains\\base.py:21\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     AsyncCallbackManager,\n\u001b[0;32m     15\u001b[0m     AsyncCallbackManagerForChainRun,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     Callbacks,\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMemory\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunInfo\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     RunnableConfig,\n\u001b[0;32m     25\u001b[0m     RunnableSerializable,\n\u001b[0;32m     26\u001b[0m     ensure_config,\n\u001b[0;32m     27\u001b[0m     run_in_executor,\n\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.memory'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from langgraph.graph import StateGraph, END\n",
    "#from crewai import Agent, Task, Crew\n",
    "#from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import TypedDict, Annotated\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c1db2-f6cf-4bd3-b68f-7974839af5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce8c4f-f97a-43ec-8343-818dceaece95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530101ab-c2ae-4e57-9b60-b19bfe7c430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create and save poker game data\n",
    "data = pd.DataFrame({\n",
    "    \"player\": [\"P1\", \"P2\", \"P3\", \"P4\"],\n",
    "    \"action\": [\"bet 1000\", \"fold\", \"raise 5000\", \"call\"],\n",
    "    \"hand\": [\"Flush\", \"Pair\", \"Straight\", \"Two Pair\"],\n",
    "    \"win\": [1, 0, 1, 0]  # 1 = win, 0 = lose\n",
    "})\n",
    "data_path = \"E://l&w/poker_logs.csv\"\n",
    "data.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e66b3-914b-4ef7-83b5-9322b1bf4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"E:\\\\l&w\\\\poker_hands.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85326d59-9e0f-4a3b-a90b-eb4fe9839cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee828ce-e1a4-424a-aa60-63e417b03d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Setup RAG for hand rankings (using Mistral)\n",
    "reader = PyPDF2.PdfReader(pdf_path)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "documents = text_splitter.split_text(text)\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "vector_store = FAISS.from_texts(documents, embeddings)\n",
    "llm_mistral = Ollama(model=\"mistral\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_mistral, chain_type=\"stuff\", retriever=vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1d1c3-ec9d-436f-8a3b-ac3f1c467ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the state for LangGraph\n",
    "class PokerState(TypedDict):\n",
    "    data: str\n",
    "    win_prob: float\n",
    "    hand_rank: str\n",
    "    strategy: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69724a11-1bb6-47f1-aa6b-b57c5263ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1: Data Analyst (processes game data, uses Mistral)\n",
    "def analyze_data(state: PokerState) -> PokerState:\n",
    "    df = pd.read_csv(data_path)\n",
    "    llm = Ollama(model=\"mistral\")\n",
    "    state[\"data\"] = llm.invoke(f\"Summarize this poker game data:\\n{df.to_string()}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57396b9-cffb-4e25-83e6-4e081ce30b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 2: ML Predictor (predicts win probability)\n",
    "def predict_win(state: PokerState) -> PokerState:\n",
    "    df = pd.read_csv(data_path)\n",
    "    X = pd.get_dummies(df[[\"action\", \"hand\"]])\n",
    "    y = df[\"win\"]\n",
    "    model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    sample = pd.get_dummies(pd.DataFrame({\"action\": [\"raise 5000\"], \"hand\": [\"Straight\"]}))\n",
    "    sample = sample.reindex(columns=X.columns, fill_value=0)\n",
    "    state[\"win_prob\"] = model.predict_proba(sample)[0][1]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82668a76-74e1-49af-acf8-907dc7a52964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 3: Hand Ranker (uses RAG with Mistral)\n",
    "def rank_hand(state: PokerState) -> PokerState:\n",
    "    query = \"What is the rank of a Straight compared to a Flush?\"\n",
    "    state[\"hand_rank\"] = qa_chain.run(query)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0feea-72cd-48f1-bc34-b608ef6871ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 4: Suggest Strategy (using LangGraph, not CrewAI)\n",
    "def suggest_strategy(state: PokerState) -> PokerState:\n",
    "    llm = Ollama(model=\"llama3.1\")  # LLaMA for reasoning\n",
    "    strategy_prompt = f\"Player has a Straight, win probability is {state['win_prob']:.2f}. Suggest a strategy.\"\n",
    "    state[\"strategy\"] = llm.invoke(strategy_prompt)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1a4ef-1171-415d-9f11-dffc3c61176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 5: Strategy Review (using LangGraph, not CrewAI)\n",
    "def review_strategy(state: PokerState) -> PokerState:\n",
    "    llm = Ollama(model=\"llama3.1\")\n",
    "    review_prompt = f\"Suggested strategy: {state['strategy']}. Win probability: {state['win_prob']:.2f}. Refine and review this strategy.\"\n",
    "    state[\"strategy\"] = llm.invoke(review_prompt)\n",
    "    return state\n",
    "\n",
    "# Build the LangGraph workflow\n",
    "workflow = StateGraph(PokerState)\n",
    "workflow.add_node(\"analyze_data\", analyze_data)\n",
    "workflow.add_node(\"predict_win\", predict_win)\n",
    "workflow.add_node(\"rank_hand\", rank_hand)\n",
    "workflow.add_node(\"suggest_strategy\", suggest_strategy)\n",
    "workflow.add_node(\"review_strategy\", review_strategy)\n",
    "workflow.add_edge(\"analyze_data\", \"predict_win\")\n",
    "workflow.add_edge(\"predict_win\", \"rank_hand\")\n",
    "workflow.add_edge(\"rank_hand\", \"suggest_strategy\")\n",
    "workflow.add_edge(\"suggest_strategy\", \"review_strategy\")\n",
    "workflow.add_edge(\"review_strategy\", END)\n",
    "workflow.set_entry_point(\"analyze_data\")\n",
    "\n",
    "# Run the workflow\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\"data\": \"\", \"win_prob\": 0.0, \"hand_rank\": \"\", \"strategy\": \"\"})\n",
    "\n",
    "print(\"Project Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1ec4f-bfb6-4540-b963-46c12b9312fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Agent 4: Strategy Crew (CrewAI team to suggest strategy, uses LLaMA)\n",
    "def suggest_strategy(state: PokerState) -> PokerState:\n",
    "    strategist = Agent(\n",
    "        role=\"Poker Strategist\",\n",
    "        goal=\"Suggest a betting strategy\",\n",
    "        backstory=\"Experienced poker player\",\n",
    "        llm=Ollama(model=\"llama3.1\")  # LLaMA for reasoning\n",
    "    )\n",
    "    strategy_task = Task(\n",
    "        description=f\"Player has a Straight, win probability is {state['win_prob']:.2f}. Suggest a strategy.\",\n",
    "        agent=strategist,\n",
    "        expected_output=\"A betting strategy suggestion\"\n",
    "    )\n",
    "    crew = Crew(agents=[strategist], tasks=[strategy_task], verbose=True)\n",
    "    state[\"strategy\"] = crew.kickoff()\n",
    "    return state'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46b801-d294-4b7a-91b2-bbf40f73c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Agent 5: AutoGen Discussion (agents discuss the strategy, uses LLaMA)\n",
    "def discuss_strategy(state: PokerState) -> PokerState:\n",
    "    config_list = {\"model\": \"llama3.1\", \"api_type\": \"ollama\", \"base_url\": \"http://localhost:11434\"}\n",
    "    user_proxy = UserProxyAgent(name=\"Player\", human_in_the_loop=False, llm_config=config_list)\n",
    "    reviewer = AssistantAgent(\n",
    "        name=\"Reviewer\",\n",
    "        system_message=\"Review and refine poker strategies.\",\n",
    "        llm_config=config_list\n",
    "    )\n",
    "    user_proxy.initiate_chat(\n",
    "        reviewer,\n",
    "        message=f\"Suggested strategy: {state['strategy']}. Win probability: {state['win_prob']:.2f}. Refine this strategy.\"\n",
    "    )\n",
    "    return state'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9275297-b940-4964-a78d-a014336c0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LangGraph workflow\n",
    "workflow = StateGraph(PokerState)\n",
    "workflow.add_node(\"analyze_data\", analyze_data)\n",
    "workflow.add_node(\"predict_win\", predict_win)\n",
    "workflow.add_node(\"rank_hand\", rank_hand)\n",
    "workflow.add_node(\"suggest_strategy\", suggest_strategy)\n",
    "workflow.add_node(\"discuss_strategy\", discuss_strategy)\n",
    "workflow.add_edge(\"analyze_data\", \"predict_win\")\n",
    "workflow.add_edge(\"predict_win\", \"rank_hand\")\n",
    "workflow.add_edge(\"rank_hand\", \"suggest_strategy\")\n",
    "workflow.add_edge(\"suggest_strategy\", \"discuss_strategy\")\n",
    "workflow.add_edge(\"discuss_strategy\", END)\n",
    "workflow.set_entry_point(\"analyze_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82829c46-739d-4691-97e4-652d45321ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\"data\": \"\", \"win_prob\": 0.0, \"hand_rank\": \"\", \"strategy\": \"\"})\n",
    "print(\"Project Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7329e16c-2dc5-416d-a7f7-b65609e3ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed374c61-cb4e-4937-add3-ca6590ee8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Streamlit and other dependencies\n",
    "!pip install streamlit langgraph crewai autogen scikit-learn\n",
    "!conda install -c conda-forge langchain langchain-community pypdf2 faiss-cpu -y\n",
    "!pip install -U langchain-ollama\n",
    "\n",
    "# Ensure Ollama is running (run in a terminal: ollama serve)\n",
    "# Verify Mistral and LLaMA are available\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf76fc-683f-4e45-8fd7-5b1fd74b9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langgraph.graph import StateGraph, END\n",
    "from crewai import Agent, Task, Crew\n",
    "from autogen import Assistant, UserProxyAgent, config_list_from_json\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import TypedDict, Annotated\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "# Streamlit app title and description\n",
    "st.title(\"Poker Analysis Assistant\")\n",
    "st.write(\"Input your poker game details to get AI-driven insights, predictions, and strategies.\")\n",
    "\n",
    "# User inputs\n",
    "player_hand = st.selectbox(\"Your Hand\", [\"Flush\", \"Straight\", \"Two Pair\", \"Pair\", \"High Card\"])\n",
    "opponent_action = st.selectbox(\"Opponent's Action\", [\"bet 1000\", \"raise 5000\", \"fold\", \"call\"])\n",
    "if st.button(\"Analyze\"):\n",
    "\n",
    "    # Step 1: Create and save poker game data\n",
    "    data = pd.DataFrame({\n",
    "        \"player\": [\"P1\", \"P2\", \"P3\", \"P4\"],\n",
    "        \"action\": [\"bet 1000\", \"fold\", \"raise 5000\", \"call\"],\n",
    "        \"hand\": [\"Flush\", \"Pair\", \"Straight\", \"Two Pair\"],\n",
    "        \"win\": [1, 0, 1, 0]\n",
    "    })\n",
    "    data_path = \"E:\\\\AI_Training\\\\poker_logs.csv\"\n",
    "    data.to_csv(data_path, index=False)\n",
    "\n",
    "    # Step 2: Create a PDF with poker hand rankings for RAG\n",
    "    pdf_path = \"E:\\\\AI_Training\\\\poker_hands.pdf\"\n",
    "    with open(pdf_path, \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "        Poker Hand Rankings\n",
    "        1. Royal Flush: Ace, King, Queen, Jack, 10, all same suit.\n",
    "        2. Straight Flush: Five consecutive cards, same suit.\n",
    "        3. Four of a Kind: Four cards of the same rank.\n",
    "        4. Full House: Three of a kind and a pair.\n",
    "        5. Flush: Five cards of the same suit.\n",
    "        6. Straight: Five consecutive cards, different suits.\n",
    "        7. Three of a Kind: Three cards of the same rank.\n",
    "        8. Two Pair: Two different pairs.\n",
    "        9. One Pair: Two cards of the same rank.\n",
    "        10. High Card: Highest card wins if no other hand is made.\n",
    "        \"\"\")\n",
    "\n",
    "    # Step 3: Setup RAG for hand rankings (using Mistral)\n",
    "    reader = PyPDF2.PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "    documents = text_splitter.split_text(text)\n",
    "    embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "    vector_store = FAISS.from_texts(documents, embeddings)\n",
    "    llm_mistral = Ollama(model=\"mistral\")\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm_mistral, chain_type=\"stuff\", retriever=vector_store.as_retriever())\n",
    "\n",
    "    # Step 4: Define the state for LangGraph\n",
    "    class PokerState(TypedDict):\n",
    "        data: str\n",
    "        win_prob: float\n",
    "        hand_rank: str\n",
    "        strategy: str\n",
    "\n",
    "    # Agent 1: Data Analyst (processes game data, uses Mistral)\n",
    "    def analyze_data(state: PokerState) -> PokerState:\n",
    "        df = pd.read_csv(data_path)\n",
    "        llm = Ollama(model=\"mistral\")\n",
    "        state[\"data\"] = llm.invoke(f\"Summarize this poker game data:\\n{df.to_string()}\")\n",
    "        return state\n",
    "\n",
    "    # Agent 2: ML Predictor (predicts win probability)\n",
    "    def predict_win(state: PokerState) -> PokerState:\n",
    "        df = pd.read_csv(data_path)\n",
    "        X = pd.get_dummies(df[[\"action\", \"hand\"]])\n",
    "        y = df[\"win\"]\n",
    "        model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        sample = pd.get_dummies(pd.DataFrame({\"action\": [opponent_action], \"hand\": [player_hand]}))\n",
    "        sample = sample.reindex(columns=X.columns, fill_value=0)\n",
    "        state[\"win_prob\"] = model.predict_proba(sample)[0][1]\n",
    "        return state\n",
    "\n",
    "    # Agent 3: Hand Ranker (uses RAG with Mistral)\n",
    "    def rank_hand(state: PokerState) -> PokerState:\n",
    "        query = f\"What is the rank of a {player_hand} compared to a Flush?\"\n",
    "        state[\"hand_rank\"] = qa_chain.run(query)\n",
    "        return state\n",
    "\n",
    "    # Agent 4: Strategy Crew (CrewAI team to suggest strategy, uses LLaMA)\n",
    "    def suggest_strategy(state: PokerState) -> PokerState:\n",
    "        strategist = Agent(\n",
    "            role=\"Poker Strategist\",\n",
    "            goal=\"Suggest a betting strategy\",\n",
    "            backstory=\"Experienced poker player\",\n",
    "            llm=Ollama(model=\"llama3.1\")\n",
    "        )\n",
    "        strategy_task = Task(\n",
    "            description=f\"Player has a {player_hand}, win probability is {state['win_prob']:.2f}. Suggest a strategy.\",\n",
    "            agent=strategist,\n",
    "            expected_output=\"A betting strategy suggestion\"\n",
    "        )\n",
    "        crew = Crew(agents=[strategist], tasks=[strategy_task], verbose=True)\n",
    "        state[\"strategy\"] = crew.kickoff()\n",
    "        return state\n",
    "\n",
    "    # Agent 5: AutoGen Discussion (agents discuss the strategy, uses LLaMA)\n",
    "    def discuss_strategy(state: PokerState) -> PokerState:\n",
    "        config_list = [{\"model\": \"llama3.1\", \"api_type\": \"ollama\", \"base_url\": \"http://localhost:11434\"}]\n",
    "        user_proxy = UserProxyAgent(name=\"Player\", human_in_the_loop=False, llm_config=config_list)\n",
    "        reviewer = Assistant(\n",
    "            name=\"Reviewer\",\n",
    "            system_message=\"Review and refine poker strategies.\",\n",
    "            llm_config=config_list\n",
    "        )\n",
    "        user_proxy.initiate_chat(\n",
    "            reviewer,\n",
    "            message=f\"Suggested strategy: {state['strategy']}. Win probability: {state['win_prob']:.2f}. Refine this strategy.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    # Build the LangGraph workflow\n",
    "    workflow = StateGraph(PokerState)\n",
    "    workflow.add_node(\"analyze_data\", analyze_data)\n",
    "    workflow.add_node(\"predict_win\", predict_win)\n",
    "    workflow.add_node(\"rank_hand\", rank_hand)\n",
    "    workflow.add_node(\"suggest_strategy\", suggest_strategy)\n",
    "    workflow.add_node(\"discuss_strategy\", discuss_strategy)\n",
    "    workflow.add_edge(\"analyze_data\", \"predict_win\")\n",
    "    workflow.add_edge(\"predict_win\", \"rank_hand\")\n",
    "    workflow.add_edge(\"rank_hand\", \"suggest_strategy\")\n",
    "    workflow.add_edge(\"suggest_strategy\", \"discuss_strategy\")\n",
    "    workflow.add_edge(\"discuss_strategy\", END)\n",
    "    workflow.set_entry_point(\"analyze_data\")\n",
    "\n",
    "    # Run the workflow\n",
    "    app = workflow.compile()\n",
    "    result = app.invoke({\"data\": \"\", \"win_prob\": 0.0, \"hand_rank\": \"\", \"strategy\": \"\"})\n",
    "\n",
    "    # Display results\n",
    "    st.subheader(\"Analysis Results\")\n",
    "    st.write(\"**Game Data Summary:**\", result[\"data\"])\n",
    "    st.write(\"**Win Probability:**\", f\"{result['win_prob']:.2f}\")\n",
    "    st.write(\"**Hand Rank Comparison:**\", result[\"hand_rank\"])\n",
    "    st.write(\"**Suggested Strategy:**\", result[\"strategy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101bfcc-0e72-4f22-99d3-daea6108de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tools and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de10663-7346-4997-8003-6429d37ac057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langgraph.graph import StateGraph, END\n",
    "from crewai import Agent, Task, Crew\n",
    "from autogen import Assistant, UserProxyAgent, config_list_from_json\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import TypedDict, Annotated\n",
    "import PyPDF2\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Streamlit app title and description\n",
    "st.title(\"Poker Analysis Assistant\")\n",
    "st.write(\"Input your poker game details to get AI-driven insights, predictions, and strategies, enhanced with tools and web APIs.\")\n",
    "\n",
    "# User inputs\n",
    "player_hand = st.selectbox(\"Your Hand\", [\"Flush\", \"Straight\", \"Two Pair\", \"Pair\", \"High Card\"])\n",
    "opponent_action = st.selectbox(\"Opponent's Action\", [\"bet 1000\", \"raise 5000\", \"fold\", \"call\"])\n",
    "if st.button(\"Analyze\"):\n",
    "\n",
    "    # Step 1: Create and save poker game data\n",
    "    data = pd.DataFrame({\n",
    "        \"player\": [\"P1\", \"P2\", \"P3\", \"P4\"],\n",
    "        \"action\": [\"bet 1000\", \"fold\", \"raise 5000\", \"call\"],\n",
    "        \"hand\": [\"Flush\", \"Pair\", \"Straight\", \"Two Pair\"],\n",
    "        \"win\": [1, 0, 1, 0]\n",
    "    })\n",
    "    data_path = \"E:\\\\AI_Training\\\\poker_logs.csv\"\n",
    "    data.to_csv(data_path, index=False)\n",
    "\n",
    "    # Step 2: Create a PDF with poker hand rankings for RAG\n",
    "    pdf_path = \"E:\\\\AI_Training\\\\poker_hands.pdf\"\n",
    "    with open(pdf_path, \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "        Poker Hand Rankings\n",
    "        1. Royal Flush: Ace, King, Queen, Jack, 10, all same suit.\n",
    "        2. Straight Flush: Five consecutive cards, same suit.\n",
    "        3. Four of a Kind: Four cards of the same rank.\n",
    "        4. Full House: Three of a kind and a pair.\n",
    "        5. Flush: Five cards of the same suit.\n",
    "        6. Straight: Five consecutive cards, different suits.\n",
    "        7. Three of a Kind: Three cards of the same rank.\n",
    "        8. Two Pair: Two different pairs.\n",
    "        9. One Pair: Two cards of the same rank.\n",
    "        10. High Card: Highest card wins if no other hand is made.\n",
    "        \"\"\")\n",
    "\n",
    "    # Step 3: Setup RAG for hand rankings (using Mistral)\n",
    "    reader = PyPDF2.PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "    documents = text_splitter.split_text(text)\n",
    "    embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "    vector_store = FAISS.from_texts(documents, embeddings)\n",
    "    llm_mistral = Ollama(model=\"mistral\")\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm_mistral, chain_type=\"stuff\", retriever=vector_store.as_retriever())\n",
    "\n",
    "    # Step 4: Define the state for LangGraph\n",
    "    class PokerState(TypedDict):\n",
    "        data: str\n",
    "        win_prob: float\n",
    "        hand_rank: str\n",
    "        strategy: str\n",
    "        odds: float\n",
    "        historical_win_rate: float\n",
    "\n",
    "    # Custom Function: Calculate poker odds\n",
    "    def calculate_odds(hand: str) -> float:\n",
    "        odds_dict = {\n",
    "            \"Flush\": 0.75,\n",
    "            \"Straight\": 0.65,\n",
    "            \"Two Pair\": 0.50,\n",
    "            \"Pair\": 0.30,\n",
    "            \"High Card\": 0.10\n",
    "        }\n",
    "        return odds_dict.get(hand, 0.10)\n",
    "\n",
    "    # Mock Web API: Fetch historical win rate for a hand\n",
    "    def fetch_historical_win_rate(hand: str) -> float:\n",
    "        # Simulate an API call (in real-world, use requests.get() with a poker stats API)\n",
    "        # Mock response: Historical win rates for hands\n",
    "        win_rate_dict = {\n",
    "            \"Flush\": 0.82,\n",
    "            \"Straight\": 0.70,\n",
    "            \"Two Pair\": 0.55,\n",
    "            \"Pair\": 0.35,\n",
    "            \"High Card\": 0.15\n",
    "        }\n",
    "        return win_rate_dict.get(hand, 0.15)\n",
    "\n",
    "    # Agent 1: Data Analyst (processes game data, uses Mistral)\n",
    "    def analyze_data(state: PokerState) -> PokerState:\n",
    "        df = pd.read_csv(data_path)\n",
    "        llm = Ollama(model=\"mistral\")\n",
    "        state[\"data\"] = llm.invoke(f\"Summarize this poker game data:\\n{df.to_string()}\")\n",
    "        return state\n",
    "\n",
    "    # Agent 2: ML Predictor (predicts win probability)\n",
    "    def predict_win(state: PokerState) -> PokerState:\n",
    "        df = pd.read_csv(data_path)\n",
    "        X = pd.get_dummies(df[[\"action\", \"hand\"]])\n",
    "        y = df[\"win\"]\n",
    "        model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        sample = pd.get_dummies(pd.DataFrame({\"action\": [opponent_action], \"hand\": [player_hand]}))\n",
    "        sample = sample.reindex(columns=X.columns, fill_value=0)\n",
    "        state[\"win_prob\"] = model.predict_proba(sample)[0][1]\n",
    "        return state\n",
    "\n",
    "    # Agent 3: Hand Ranker (uses RAG with Mistral)\n",
    "    def rank_hand(state: PokerState) -> PokerState:\n",
    "        query = f\"What is the rank of a {player_hand} compared to a Flush?\"\n",
    "        state[\"hand_rank\"] = qa_chain.run(query)\n",
    "        return state\n",
    "\n",
    "    # Agent 4: Odds Calculator (uses custom function and web API)\n",
    "    def calculate_odds_and_stats(state: PokerState) -> PokerState:\n",
    "        # Use custom function to calculate odds\n",
    "        state[\"odds\"] = calculate_odds(player_hand)\n",
    "        # Use mock web API to fetch historical win rate\n",
    "        state[\"historical_win_rate\"] = fetch_historical_win_rate(player_hand)\n",
    "        return state\n",
    "\n",
    "    # Agent 5: Strategy Crew (CrewAI team to suggest strategy, uses LLaMA)\n",
    "    def suggest_strategy(state: PokerState) -> PokerState:\n",
    "        strategist = Agent(\n",
    "            role=\"Poker Strategist\",\n",
    "            goal=\"Suggest a betting strategy\",\n",
    "            backstory=\"Experienced poker player\",\n",
    "            llm=Ollama(model=\"llama3.1\")\n",
    "        )\n",
    "        strategy_task = Task(\n",
    "            description=f\"Player has a {player_hand}, win probability is {state['win_prob']:.2f}, odds are {state['odds']:.2f}, historical win rate is {state['historical_win_rate']:.2f}. Suggest a strategy.\",\n",
    "            agent=strategist,\n",
    "            expected_output=\"A betting strategy suggestion\"\n",
    "        )\n",
    "        crew = Crew(agents=[strategist], tasks=[strategy_task], verbose=True)\n",
    "        state[\"strategy\"] = crew.kickoff()\n",
    "        return state\n",
    "\n",
    "    # Agent 6: AutoGen Discussion (agents discuss the strategy, uses LLaMA)\n",
    "    def discuss_strategy(state: PokerState) -> PokerState:\n",
    "        config_list = [{\"model\": \"llama3.1\", \"api_type\": \"ollama\", \"base_url\": \"http://localhost:11434\"}]\n",
    "        user_proxy = UserProxyAgent(name=\"Player\", human_in_the_loop=False, llm_config=config_list)\n",
    "        reviewer = Assistant(\n",
    "            name=\"Reviewer\",\n",
    "            system_message=\"Review and refine poker strategies.\",\n",
    "            llm_config=config_list\n",
    "        )\n",
    "        user_proxy.initiate_chat(\n",
    "            reviewer,\n",
    "            message=f\"Suggested strategy: {state['strategy']}. Win probability: {state['win_prob']:.2f}. Odds: {state['odds']:.2f}. Historical win rate: {state['historical_win_rate']:.2f}. Refine this strategy.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    # Build the LangGraph workflow\n",
    "    workflow = StateGraph(PokerState)\n",
    "    workflow.add_node(\"analyze_data\", analyze_data)\n",
    "    workflow.add_node(\"predict_win\", predict_win)\n",
    "    workflow.add_node(\"rank_hand\", rank_hand)\n",
    "    workflow.add_node(\"calculate_odds_and_stats\", calculate_odds_and_stats)\n",
    "    workflow.add_node(\"suggest_strategy\", suggest_strategy)\n",
    "    workflow.add_node(\"discuss_strategy\", discuss_strategy)\n",
    "    workflow.add_edge(\"analyze_data\", \"predict_win\")\n",
    "    workflow.add_edge(\"predict_win\", \"rank_hand\")\n",
    "    workflow.add_edge(\"rank_hand\", \"calculate_odds_and_stats\")\n",
    "    workflow.add_edge(\"calculate_odds_and_stats\", \"suggest_strategy\")\n",
    "    workflow.add_edge(\"suggest_strategy\", \"discuss_strategy\")\n",
    "    workflow.add_edge(\"discuss_strategy\", END)\n",
    "    workflow.set_entry_point(\"analyze_data\")\n",
    "\n",
    "    # Run the workflow\n",
    "    app = workflow.compile()\n",
    "    result = app.invoke({\"data\": \"\", \"win_prob\": 0.0, \"hand_rank\": \"\", \"strategy\": \"\", \"odds\": 0.0, \"historical_win_rate\": 0.0})\n",
    "\n",
    "    # Display results\n",
    "    st.subheader(\"Analysis Results\")\n",
    "    st.write(\"**Game Data Summary:**\", result[\"data\"])\n",
    "    st.write(\"**Win Probability (ML Prediction):**\", f\"{result['win_prob']:.2f}\")\n",
    "    st.write(\"**Hand Rank Comparison (RAG):**\", result[\"hand_rank\"])\n",
    "    st.write(\"**Calculated Odds (Custom Function):**\", f\"{result['odds']:.2f}\")\n",
    "    st.write(\"**Historical Win Rate (Web API):**\", f\"{result['historical_win_rate']:.2f}\")\n",
    "    st.write(\"**Suggested Strategy (Agentic AI):**\", result[\"strategy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38edf1-bcce-412f-8a0e-50a0ef3f34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://openweathermap.org/city/2643743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f119929-914d-4da6-a667-b4c317d690ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "# Streamlit app title and description\n",
    "st.title(\"Weather-Based Poker Recommendation\")\n",
    "st.write(\"Check if today’s weather in LA is good for a poker game!\")\n",
    "\n",
    "# OpenWeatherMap API setup\n",
    "API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your OpenWeatherMap API key\n",
    "CITY = \"Los Angeles\"\n",
    "BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "# Function to fetch weather data\n",
    "def get_weather(city):\n",
    "    params = {\"q\": city, \"appid\": API_KEY, \"units\": \"metric\"}  # Metric for Celsius\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        if response.status_code == 200:\n",
    "            temp = data[\"main\"][\"temp\"]\n",
    "            condition = data[\"weather\"][0][\"main\"]\n",
    "            return temp, condition\n",
    "        else:\n",
    "            return None, f\"Error: {data['message']}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Error fetching weather: {str(e)}\"\n",
    "\n",
    "# Function to recommend poker game based on weather\n",
    "def recommend_poker_game(temp, condition):\n",
    "    if temp is None:\n",
    "        return \"Unable to make a recommendation due to weather fetch error.\"\n",
    "    if temp > 30 or condition.lower() in [\"rain\", \"thunderstorm\"]:\n",
    "        return \"Weather is too hot or rainy. Consider playing poker indoors!\"\n",
    "    elif 15 <= temp <= 30 and condition.lower() in [\"clear\", \"clouds\"]:\n",
    "        return \"Weather is great! It’s a perfect day for a poker game outdoors!\"\n",
    "    else:\n",
    "        return \"Weather is okay. You can play poker, but indoor might be more comfortable.\"\n",
    "\n",
    "# Fetch weather and make recommendation\n",
    "if st.button(\"Check Weather and Recommendation\"):\n",
    "    temp, condition = get_weather(CITY)\n",
    "    recommendation = recommend_poker_game(temp, condition)\n",
    "    \n",
    "    # Display results\n",
    "    st.subheader(\"Weather in LA\")\n",
    "    if temp is not None:\n",
    "        st.write(f\"Temperature: {temp}°C\")\n",
    "        st.write(f\"Condition: {condition}\")\n",
    "    else:\n",
    "        st.write(condition)  # Error message\n",
    "    st.subheader(\"Recommendation\")\n",
    "    st.write(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e8d7c-24d6-4093-8ac1-74dad5a78f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbab2f-3a3c-4a5a-8ee8-91dfe4878e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e371a-3f1e-42f9-af00-dd13e27e460d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc891d1-d92f-469c-b2ed-5abdda84cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.llms import Ollama\n",
    "from typing import TypedDict, Annotated\n",
    "import pandas as pd\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the state (data that agents share)\n",
    "class GameState(TypedDict):\n",
    "    raw_data: str\n",
    "    summary: str\n",
    "\n",
    "# Create sample poker game data\n",
    "data = pd.DataFrame({\n",
    "    \"player\": [\"P1\", \"P2\", \"P3\"],\n",
    "    \"action\": [\"bet 1000\", \"fold\", \"raise 5000\"],\n",
    "    \"hand\": [\"Flush\", \"Pair\", \"Straight\"]\n",
    "})\n",
    "data_path = \"E:\\\\AI_Training\\\\poker_logs.csv\"\n",
    "data.to_csv(data_path, index=False)\n",
    "\n",
    "# Agent 1: Load and process data\n",
    "def process_data(state: GameState) -> GameState:\n",
    "    df = pd.read_csv(data_path)\n",
    "    state[\"raw_data\"] = df.to_string()\n",
    "    return state\n",
    "\n",
    "# Agent 2: Summarize data using Mistral\n",
    "def summarize_data(state: GameState) -> GameState:\n",
    "    llm = Ollama(model=\"mistral\")\n",
    "    prompt = f\"Summarize this poker game data:\\n{state['raw_data']}\"\n",
    "    state[\"summary\"] = llm.invoke(prompt)\n",
    "    return state\n",
    "\n",
    "# Build the workflow\n",
    "workflow = StateGraph(GameState)\n",
    "workflow.add_node(\"process_data\", process_data)\n",
    "workflow.add_node(\"summarize_data\", summarize_data)\n",
    "workflow.add_edge(\"process_data\", \"summarize_data\")\n",
    "workflow.add_edge(\"summarize_data\", END)\n",
    "workflow.set_entry_point(\"process_data\")\n",
    "\n",
    "# Visualize the workflow\n",
    "def visualize_workflow(workflow):\n",
    "    dot = Digraph(comment=\"LangGraph Workflow\")\n",
    "    dot.attr(rankdir=\"LR\")  # Left to right layout\n",
    "\n",
    "    # Add nodes\n",
    "    for node in workflow.nodes:\n",
    "        dot.node(node, node)\n",
    "\n",
    "    # Add entry point\n",
    "    dot.node(\"START\", \"START\", shape=\"circle\")\n",
    "    dot.edge(\"START\", workflow.entry_point)\n",
    "\n",
    "    # Add edges\n",
    "    for edge in workflow.edges:\n",
    "        dot.edge(edge[0], edge[1])\n",
    "\n",
    "    # Add END node\n",
    "    dot.node(\"END\", \"END\", shape=\"doublecircle\")\n",
    "    return dot\n",
    "\n",
    "# Display the graph\n",
    "graph = visualize_workflow(workflow)\n",
    "display(graph)\n",
    "\n",
    "# Run the workflow\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\"raw_data\": \"\", \"summary\": \"\"})\n",
    "print(\"LangGraph Result - Summary:\", result[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a3713-9ae0-48fd-bda3-a86bf3169dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this near the top of poker_app.py, after imports\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Add this before \"Run the workflow\" (around line 185 in the previous poker_app.py)\n",
    "# Visualize the workflow\n",
    "def visualize_workflow(workflow):\n",
    "    dot = Digraph(comment=\"LangGraph Workflow\")\n",
    "    dot.attr(rankdir=\"LR\")\n",
    "\n",
    "    # Add nodes\n",
    "    for node in workflow.nodes:\n",
    "        dot.node(node, node)\n",
    "\n",
    "    # Add entry point\n",
    "    dot.node(\"START\", \"START\", shape=\"circle\")\n",
    "    dot.edge(\"START\", workflow.entry_point)\n",
    "\n",
    "    # Add edges\n",
    "    for edge in workflow.edges:\n",
    "        dot.edge(edge[0], edge[1])\n",
    "\n",
    "    # Add END node\n",
    "    dot.node(\"END\", \"END\", shape=\"doublecircle\")\n",
    "\n",
    "    # Save the graph as a PNG\n",
    "    dot.format = \"png\"\n",
    "    dot.render(\"E:\\\\AI_Training\\\\workflow_graph\", view=False)\n",
    "    return \"E:\\\\AI_Training\\\\workflow_graph.png\"\n",
    "\n",
    "# Add this after \"Build the LangGraph workflow\" (around line 183)\n",
    "graph_path = visualize_workflow(workflow)\n",
    "\n",
    "# Add this to the \"Display results\" section (around line 193)\n",
    "st.subheader(\"Workflow Visualization\")\n",
    "st.image(graph_path, caption=\"LangGraph Workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fcb4fc-c28a-4c2e-8f15-958599b63bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
